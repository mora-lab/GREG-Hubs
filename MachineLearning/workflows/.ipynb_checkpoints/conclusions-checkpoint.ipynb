{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Conclusions on the runs of Logistic Regression and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = 'justify'> For the linear models of Logistic Regression, we'll pick the variable that has the lowest p-value and Variance Inflation Factor (VIF). For Random Forests, we'll pick the \"most important variable\" engendered by the <b>varImp</b> function. The results are tabulated as below.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| S.No. | Algorithm | Optimum Model | Sensitivity (%) | Specificity (%) | Accuracy (%) | Cell Line | Significant Variable\n",
    "| --- | --- | --- | --- | --- | --- | --- | ---\n",
    "| 1. | Logistic Regression | a549modelSMOTE | 69.49417 | 51.4377 | 69.46981 | A549 | H3K27me3\n",
    "| 2. | Logistic Regression | h1escparetoModelSmote | 58.99516 | 53.49026 | 58.98057 | H1ESC | H3K36me3\n",
    "| 3. | Logistic Regression | helamodel1SMOTE | 75.37186 | 53.48485 | 75.34016 | HELA | RNAPol2\n",
    "| 4. | Logistic Regression | imr90paretoModelSmote | 61.66091 | 56.12334 | 60.85818 | IMR90 | H3K9me3\n",
    "| 5. | Logistic Regression | k562modelUnder1 | 72.1866 | 55.47826 | 72.14441 | K562 | H3K27me3\n",
    "| 6. | Logistic Regression | mcf7model1Under | 78.25222 | 54.38043 | 78.12211 | MCF7 | RNA.Seq\n",
    "| 7. | Random Forests | a549rfSmote | 95.69667 | 34.30079 | 95.62169 | A549 | RNAPol2\n",
    "| 8. | Random Forests | h1escrf1 | 99.26789 | 52.55973 | 99.13568 | H1ESC | H3K27me3\n",
    "| 9. | Random Forests | helarf1 | 97.23307 | 46.12069 | 97.15515 | HELA | RNAPol2\n",
    "| 10. | Random Forests | imr90rf1 | 89.32611 | 79.01205 | 87.82932 | IMR90 | H3K27me3\n",
    "| 11. | Random Forests | k562rf1 | 99.52067 | 86.23853 | 99.48906 | K562 | H3K4me1\n",
    "| 12. | Random Forests | mcf7rf1 | 97.63652 | 76.89394 | 97.51932 | MCF7 | RAD21\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significant Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"justify\"> The significant models acknowledge H3K27me3, which is revered mark for transcriptional repression, as significant variable for H1ESC and IMR90 cell-lines. RAD21 was found pertinent in the MCF7 cell line, which is responsible for regulating structure and organization of the chromosomes. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"justify\"> Additionally, a machine learning model learns to distinguish between “Hubs” and “Non-Hubs” based on the patterns in the data. When the model is supplied with a fresh instance, based on its training, it will grade the corresponding class as “Hub” or “Non-Hub”. There are no specifics, at this instance of analysis, that tell us which features/ variables better predict “Hubs” and which “Non-Hubs”. There are just some “general” significant variables that mark a prediction as either “Hub” or “Non-Hub”. This notion drew towards the clustering algorithms that can help identify key variables in each cluster; theoretically, there must be two- \"Hubs\" and \"Non-Hubs. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align= \"justify\"> The model is generally evaluated based on its performance on the test data. It was found that a 70:30 partition worked best for building and testing the model, respectively. Additionally, the SMOTE transformation of the 70% train data was sourced to build the model. This brought about parity into the model, vis-à-vis the distribution of classes. In all cell-types, the purported \"Non-Hubs\" were far larger in number than \"Hubs\" (upto a 1000 fold). This had to be taken into account, particularly while training the model. </p>\n",
    "\n",
    "<p align = \"justify\"> The accuracy of the model is always in conjuction to the data we are testing the model upon. In our case, a general portfolio of the test data (throughout cell-types) has been (again) quite imbalanced. So, the higher/ lower accuracy, generally, does not necessarily translate into success; it rather depends on the exactness of positive and negative classes in the data. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let us observe the class distribution for A549 cell-line (test) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "     0      1 \n",
       "463531    626 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "require(\"RCurl\")\n",
    "a549TestLR <- read.table(textConnection(getURL(\"http://www.moralab.science/downloads/Chromatin-Hubs/buildData/optimalModelTestA549LR.txt\")), \n",
    "sep = \" \", header = TRUE)\n",
    "table(a549TestLR$Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "     0      1 \n",
       "309946    379 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a549TestRF <- read.table(textConnection(getURL(\"http://www.moralab.science/downloads/Chromatin-Hubs/buildData/optimalModelTestA549RF.txt\")), \n",
    "sep = \" \", header = TRUE)\n",
    "table(a549TestRF$Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hubs in the test data for the optimal logistic regression model are about .1 % of the entire data. If the model is able to correctly identify these, then the accuracy of the model is good. Mostly, the accuracy of a model has a general inclination towards the prediction of positive classes (sensitivity; also observable in the above table). Higher sensitivity is what we seek (although, the correct estimation of negative classes is also the flip-side of the coin) and that in-turn determines the success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"justify\"> We can infer random forests models have better performance than logistic regression models. Generally, they engender higher sensitivity (ability to correctly identify the positive class: Hubs), as compared to specificity (ability to correctly identify the negative class: Non-Hubs), and that they are really good at this (high accuracy).</p>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
